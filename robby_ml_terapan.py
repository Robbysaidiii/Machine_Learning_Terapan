# -*- coding: utf-8 -*-
"""Robby_ml_terapan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YmSJ8gZe1vmzAQG1CJPSsNA61tezHR85
"""

# 1. IMPORT LIBRARIES
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score,
                           precision_score, recall_score, f1_score, roc_auc_score, roc_curve)
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
import warnings
warnings.filterwarnings("ignore")

# Set style untuk visualisasi
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

print("="*70)
print("MOVIE SUCCESS PREDICTION - MACHINE LEARNING PROJECT")
print("="*70)

"""#DATA UNDERSTANDING

## load dataset
"""

print("\n DATA LOADING DAN INITIAL EXPLORATION")
print("-"*50)

# Load dataset
file_path = '/content/Movies Recommendation.csv'
df = pd.read_csv(file_path)

print(f"Dataset shape: {df.shape}")
print(f"Dataset berhasil dimuat dengan {df.shape[0]} baris dan {df.shape[1]} kolom")

"""## informasi dataset"""

print("\nInformasi Dataset:")
df.info()

print("\n5 Baris Pertama Dataset:")
display(df.head())

print("\nStatistik Deskriptif:")
display(df.describe())

"""##analisis dataset"""

#Analisis tipe data
print("\nTipe Data Setiap Kolom:")
for col in df.columns:
    print(f"{col}: {df[col].dtype} - {df[col].nunique()} unique values")

"""#EXPLORATORY DATA ANALYSIS (EDA)

##Analisis Missing Values
"""

print("\n3. EXPLORATORY DATA ANALYSIS")
print("-"*50)

# Analisis Missing Values
print("ANALISIS MISSING VALUES:")
missing_values = df.isnull().sum()
missing_percent = (missing_values / len(df)) * 100
missing_df = pd.DataFrame({
    'Missing_Count': missing_values,
    'Missing_Percentage': missing_percent
}).sort_values('Missing_Percentage', ascending=False)

print("Missing Values per Kolom:")
display(missing_df[missing_df['Missing_Count'] > 0])

#Visualisasi Missing Values
plt.figure(figsize=(12, 6))
sns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='viridis')
plt.title('Heatmap Missing Values')
plt.tight_layout()
plt.show()

# Identifikasi kolom dengan missing values tinggi
high_missing_cols = missing_df[missing_df['Missing_Percentage'] > 50].index.tolist()
print(f"\nKolom dengan missing values >50%: {high_missing_cols}")

"""##Analisis Target Variable (Movie_Revenue)"""

print("\nANALISIS TARGET VARIABLE (MOVIE_REVENUE):")
revenue_stats = df['Movie_Revenue'].describe()
print("Statistik Revenue:")
display(revenue_stats)

#Visualisasi distribusi revenue
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Histogram revenue
axes[0,0].hist(df['Movie_Revenue'].dropna(), bins=50, edgecolor='black', alpha=0.7)
axes[0,0].set_title('Distribusi Movie Revenue')
axes[0,0].set_xlabel('Revenue (USD)')
axes[0,0].set_ylabel('Frequency')

# Log transformation
axes[0,1].hist(np.log1p(df['Movie_Revenue'].dropna()), bins=50, edgecolor='black', alpha=0.7)
axes[0,1].set_title('Distribusi Log(Movie Revenue)')
axes[0,1].set_xlabel('Log Revenue')
axes[0,1].set_ylabel('Frequency')

# Boxplot
axes[1,0].boxplot(df['Movie_Revenue'].dropna())
axes[1,0].set_title('Boxplot Movie Revenue')
axes[1,0].set_ylabel('Revenue (USD)')

# Revenue percentiles
percentiles = np.percentile(df['Movie_Revenue'].dropna(), [25, 50, 75, 90, 95])
axes[1,1].bar(['Q1', 'Median', 'Q3', 'P90', 'P95'], percentiles)
axes[1,1].set_title('Revenue Percentiles')
axes[1,1].set_ylabel('Revenue (USD)')

plt.tight_layout()
plt.show()

"""## Analisis fitur numerik"""

numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()
print(f"\nFitur Numerik: {numeric_features}")

# Correlation analysis untuk fitur numerik
numeric_df = df[numeric_features].corr()
plt.figure(figsize=(10, 8))
sns.heatmap(numeric_df, annot=True, cmap='coolwarm', center=0, square=True, linewidths=0.5)
plt.title('Correlation Matrix - Fitur Numerik')
plt.tight_layout()
plt.show()

"""#DATA PREPARATION"""

print("\n4. DATA PREPARATION")
print("-"*50)

# Buat copy dataset untuk preprocessing
df_processed = df.copy()

# LANGKAH 1: Drop kolom yang tidak relevan atau menyebabkan data leakage
print("LANGKAH 1: FEATURE SELECTION")
cols_to_drop = [
    'Movie_Homepage', 'Movie_Overview', 'Movie_Tagline',
    'Movie_Keywords', 'Movie_Cast', 'Movie_Crew',
    'Movie_Revenue'  # PENTING: Dihapus untuk mencegah data leakage
]

# Hapus kolom yang ada
existing_cols_to_drop = [col for col in cols_to_drop if col in df_processed.columns]
print(f"Menghapus kolom: {existing_cols_to_drop}")
df_processed.drop(columns=existing_cols_to_drop, inplace=True)

print(f"Shape setelah drop kolom: {df_processed.shape}")

# LANGKAH 2: Buat target variable
print("\nLANGKAH 2: PEMBUATAN TARGET VARIABLE")
# Definisi kesuksesan: film dengan revenue >= $50 juta
SUCCESS_THRESHOLD = 50000000  # $50 juta USD

# Buat label berdasarkan threshold
df_processed['Success_Label'] = df['Movie_Revenue'].apply(
    lambda x: 1 if x >= SUCCESS_THRESHOLD else 0
)

# Analisis distribusi target
target_counts = df_processed['Success_Label'].value_counts()
target_percent = df_processed['Success_Label'].value_counts(normalize=True) * 100

print(f"Threshold kesuksesan: ${SUCCESS_THRESHOLD:,}")
print("Distribusi Target:")
print(f"Tidak Sukses (0): {target_counts[0]} film ({target_percent[0]:.1f}%)")
print(f"Sukses (1): {target_counts[1]} film ({target_percent[1]:.1f}%)")

# Visualisasi target distribution
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
sns.countplot(data=df_processed, x='Success_Label')
plt.title('Distribusi Label Kesuksesan')
plt.xlabel('Label (0=Tidak Sukses, 1=Sukses)')
plt.ylabel('Jumlah Film')
for i, v in enumerate(target_counts):
    plt.text(i, v + 10, str(v), ha='center', va='bottom')

plt.subplot(1, 2, 2)
plt.pie(target_counts.values, labels=['Tidak Sukses', 'Sukses'], autopct='%1.1f%%')
plt.title('Proporsi Label Kesuksesan')

plt.tight_layout()
plt.show()

# LANGKAH 3: Handle missing values
print("\nLANGKAH 3: PENANGANAN MISSING VALUES")

# Identifikasi kolom numerik dan kategorikal
numeric_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = df_processed.select_dtypes(include=['object']).columns.tolist()

# Remove target dari numeric_cols
if 'Success_Label' in numeric_cols:
    numeric_cols.remove('Success_Label')

print(f"Kolom numerik: {numeric_cols}")
print(f"Kolom kategorikal: {categorical_cols}")

# Handle missing values untuk numerik (gunakan median)
for col in numeric_cols:
    if df_processed[col].isnull().sum() > 0:
        median_val = df_processed[col].median()
        missing_count = df_processed[col].isnull().sum()
        df_processed[col].fillna(median_val, inplace=True)
        print(f"Mengisi {missing_count} missing values pada {col} dengan median: {median_val}")

# Handle missing values untuk kategorikal (gunakan mode atau 'Unknown')
for col in categorical_cols:
    if df_processed[col].isnull().sum() > 0:
        mode_val = df_processed[col].mode()[0] if len(df_processed[col].mode()) > 0 else 'Unknown'
        missing_count = df_processed[col].isnull().sum()
        df_processed[col].fillna(mode_val, inplace=True)
        print(f"Mengisi {missing_count} missing values pada {col} dengan mode: {mode_val}")

# Drop rows dengan missing values yang tersisa
initial_rows = len(df_processed)
df_processed.dropna(inplace=True)
final_rows = len(df_processed)
dropped_rows = initial_rows - final_rows
if dropped_rows > 0:
    print(f"Menghapus {dropped_rows} baris dengan missing values tersisa")

print(f"Shape final: {df_processed.shape}")

#LANGKAH 4: Feature Engineering
print("\nLANGKAH 4: FEATURE ENGINEERING")

# Encode categorical variables
le_dict = {}
for col in categorical_cols:
    if col in df_processed.columns and len(df_processed[col].unique()) > 1:
        le = LabelEncoder()
        df_processed[col + '_encoded'] = le.fit_transform(df_processed[col].astype(str))
        le_dict[col] = le
        print(f"Encode {col}: {len(le.classes_)} kategori unik")

# Pilih fitur untuk modeling
base_features = ['Movie_Budget', 'Movie_Popularity', 'Movie_Runtime',
                'Movie_Vote', 'Movie_Vote_Count']
encoded_features = [col for col in df_processed.columns if col.endswith('_encoded')]

# Gabungkan semua fitur yang tersedia
all_features = base_features + encoded_features
available_features = [col for col in all_features if col in df_processed.columns]

print(f"Fitur yang tersedia untuk modeling: {available_features}")
print(f"Total fitur: {len(available_features)}")

# LANGKAH 5: Data Splitting dan Scaling
print("\nLANGKAH 5: DATA SPLITTING DAN SCALING")

# Prepare X dan y
X = df_processed[available_features]
y = df_processed['Success_Label']

print(f"Shape X: {X.shape}")
print(f"Shape y: {y.shape}")

# Train-test split dengan stratification
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Training set: {X_train.shape}")
print(f"Test set: {X_test.shape}")
print(f"Training target distribusi: {np.bincount(y_train)}")
print(f"Test target distribusi: {np.bincount(y_test)}")

# Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Feature scaling menggunakan StandardScaler telah diterapkan")

# Analisis korelasi fitur dengan target
correlation_with_target = X_train.corrwith(y_train).abs().sort_values(ascending=False)
print("\nKorelasi Fitur dengan Target (nilai absolut):")
display(correlation_with_target)

# Visualisasi korelasi
plt.figure(figsize=(10, 6))
correlation_with_target.plot(kind='barh')
plt.title('Korelasi Fitur dengan Target Variable')
plt.xlabel('Korelasi Absolut')
plt.tight_layout()
plt.show()

"""#MODELING"""

print("\n5. MODELING")
print("-"*50)

# Definisi model dan parameter untuk tuning
models_config = {
    'Decision Tree': {
        'model': DecisionTreeClassifier(random_state=42),
        'params': {
            'max_depth': [3, 5, 7, 10, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'criterion': ['gini', 'entropy']
        }
    },
    'Random Forest': {
        'model': RandomForestClassifier(random_state=42, n_jobs=-1),
        'params': {
            'n_estimators': [50, 100, 200],
            'max_depth': [3, 5, 7, 10, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }
    },
    'XGBoost': {
        'model': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
        'params': {
            'n_estimators': [50, 100, 200],
            'max_depth': [3, 5, 7],
            'learning_rate': [0.01, 0.1, 0.2],
            'subsample': [0.8, 0.9, 1.0]
        }
    }
}

results = {}
best_models = {}

print("BASELINE MODEL TRAINING")
print("-"*30)

# Train baseline models (tanpa tuning)
baseline_results = {}
for name, config in models_config.items():
    print(f"\nTraining baseline {name}...")
    # Train baseline model
    baseline_model = config['model']
    baseline_model.fit(X_train_scaled, y_train)

    # Predictions
    y_pred_baseline = baseline_model.predict(X_test_scaled)
    y_proba_baseline = baseline_model.predict_proba(X_test_scaled)[:, 1]

    # Calculate metrics
    baseline_acc = accuracy_score(y_test, y_pred_baseline)
    baseline_precision = precision_score(y_test, y_pred_baseline)
    baseline_recall = recall_score(y_test, y_pred_baseline)
    baseline_f1 = f1_score(y_test, y_pred_baseline)
    baseline_auc = roc_auc_score(y_test, y_proba_baseline)

    baseline_results[name] = {
        'accuracy': baseline_acc,
        'precision': baseline_precision,
        'recall': baseline_recall,
        'f1_score': baseline_f1,
        'auc_score': baseline_auc
    }

    print(f"Baseline {name} - Accuracy: {baseline_acc:.4f}, F1: {baseline_f1:.4f}, AUC: {baseline_auc:.4f}")

print("\nHYPERPARAMETER TUNING & MODEL IMPROVEMENT")
print("-"*45)

# Hyperparameter tuning untuk improvement
for name, config in models_config.items():
    print(f"\nTuning {name}...")

    # GridSearchCV untuk hyperparameter tuning
    grid_search = GridSearchCV(
        config['model'],
        config['params'],
        cv=5,
        scoring='f1',  # Fokus pada F1-score untuk balanced performance
        n_jobs=-1,
        verbose=0
    )

    # Fit dengan data training
    grid_search.fit(X_train_scaled, y_train)

    # Simpan best model
    best_models[name] = grid_search.best_estimator_

    # Cross-validation scores
    cv_scores = cross_val_score(grid_search.best_estimator_, X_train_scaled, y_train, cv=5, scoring='f1')

    # Predictions
    y_train_pred = grid_search.best_estimator_.predict(X_train_scaled)
    y_test_pred = grid_search.best_estimator_.predict(X_test_scaled)
    y_test_proba = grid_search.best_estimator_.predict_proba(X_test_scaled)[:, 1]

    # Calculate comprehensive metrics
    train_acc = accuracy_score(y_train, y_train_pred)
    test_acc = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    recall = recall_score(y_test, y_test_pred)
    f1 = f1_score(y_test, y_test_pred)
    auc = roc_auc_score(y_test, y_test_proba)

    # Store results
    results[name] = {
        'best_params': grid_search.best_params_,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'train_accuracy': train_acc,
        'test_accuracy': test_acc,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'auc_score': auc,
        'y_test_pred': y_test_pred,
        'y_test_proba': y_test_proba,
        'baseline_f1': baseline_results[name]['f1_score'],
        'improvement': f1 - baseline_results[name]['f1_score']
    }

    print(f"Best parameters: {grid_search.best_params_}")
    print(f"CV F1-Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
    print(f"Test Accuracy: {test_acc:.4f}")
    print(f"Test F1-Score: {f1:.4f}")
    print(f"Improvement dari baseline: {f1 - baseline_results[name]['f1_score']:+.4f}")

"""#EVALUATION"""

print("\n6. MODEL EVALUATION & COMPARISON")
print("-"*50)

# Buat comparison dataframe
comparison_data = []
for name in results.keys():
    comparison_data.append({
        'Model': name,
        'CV_F1_Mean': results[name]['cv_mean'],
        'CV_F1_Std': results[name]['cv_std'],
        'Train_Accuracy': results[name]['train_accuracy'],
        'Test_Accuracy': results[name]['test_accuracy'],
        'Precision': results[name]['precision'],
        'Recall': results[name]['recall'],
        'F1_Score': results[name]['f1_score'],
        'AUC_Score': results[name]['auc_score'],
        'Baseline_F1': results[name]['baseline_f1'],
        'Improvement': results[name]['improvement']
    })

comparison_df = pd.DataFrame(comparison_data)

print("PERBANDINGAN PERFORMA MODEL:")
display(comparison_df.round(4))

# Tentukan model terbaik berdasarkan F1-score
best_model_idx = comparison_df['F1_Score'].idxmax()
best_model_name = comparison_df.loc[best_model_idx, 'Model']
best_f1_score = comparison_df.loc[best_model_idx, 'F1_Score']

print(f"\nMODEL TERBAIK: {best_model_name}")
print(f"F1-Score Terbaik: {best_f1_score:.4f}")

# Analisis improvement
print("\nANALISIS IMPROVEMENT (Baseline vs Tuned):")
for name in results.keys():
    baseline_f1 = results[name]['baseline_f1']
    tuned_f1 = results[name]['f1_score']
    improvement = results[name]['improvement']
    improvement_pct = (improvement / baseline_f1) * 100

    print(f"{name}:")
    print(f"  Baseline F1: {baseline_f1:.4f}")
    print(f"  Tuned F1:    {tuned_f1:.4f}")
    print(f"  Improvement: {improvement:+.4f} ({improvement_pct:+.1f}%)")

#Detailed evaluation untuk setiap model
print("\nDETAILED EVALUATION:")
print("="*50)

for name in results.keys():
    result = results[name]
    print(f"\n{name.upper()}")
    print("-" * len(name))
    print(f"Best Parameters: {result['best_params']}")
    print(f"Cross-Validation F1: {result['cv_mean']:.4f} (+/- {result['cv_std']*2:.4f})")
    print(f"Training Accuracy: {result['train_accuracy']:.4f}")
    print(f"Test Accuracy: {result['test_accuracy']:.4f}")
    print(f"Precision: {result['precision']:.4f}")
    print(f"Recall: {result['recall']:.4f}")
    print(f"F1-Score: {result['f1_score']:.4f}")
    print(f"AUC Score: {result['auc_score']:.4f}")

    # Analisis overfitting
    train_test_diff = result['train_accuracy'] - result['test_accuracy']
    if train_test_diff > 0.05:
        print(f"⚠️  Possible overfitting (train-test diff: {train_test_diff:.4f})")
    else:
        print(f"✅ Good generalization (train-test diff: {train_test_diff:.4}")